{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:22.830885Z",
     "iopub.status.busy": "2022-08-28T10:36:22.830471Z",
     "iopub.status.idle": "2022-08-28T10:36:29.677577Z",
     "shell.execute_reply": "2022-08-28T10:36:29.676591Z",
     "shell.execute_reply.started": "2022-08-28T10:36:22.830800Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.compat.v1 import ConfigProto, InteractiveSession\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:29.680286Z",
     "iopub.status.busy": "2022-08-28T10:36:29.679617Z",
     "iopub.status.idle": "2022-08-28T10:36:51.543431Z",
     "shell.execute_reply": "2022-08-28T10:36:51.542330Z",
     "shell.execute_reply.started": "2022-08-28T10:36:29.680248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_path</th>\n",
       "      <th>Articles</th>\n",
       "      <th>Summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>politics/361.txt</td>\n",
       "      <td>Budget to set scene for election..Gordon Brown...</td>\n",
       "      <td>- Increase in the stamp duty threshold from £6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>politics/245.txt</td>\n",
       "      <td>Army chiefs in regiments decision..Military ch...</td>\n",
       "      <td>\"They are very much not for the good and will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics/141.txt</td>\n",
       "      <td>Howard denies split over ID cards..Michael How...</td>\n",
       "      <td>Michael Howard has denied his shadow cabinet w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>politics/372.txt</td>\n",
       "      <td>Observers to monitor UK election..Ministers wi...</td>\n",
       "      <td>The report said individual registration should...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>politics/333.txt</td>\n",
       "      <td>Kilroy names election seat target..Ex-chat sho...</td>\n",
       "      <td>UKIP's leader, Roger Knapman, has said he is g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          File_path                                           Articles  \\\n",
       "0  politics/361.txt  Budget to set scene for election..Gordon Brown...   \n",
       "1  politics/245.txt  Army chiefs in regiments decision..Military ch...   \n",
       "2  politics/141.txt  Howard denies split over ID cards..Michael How...   \n",
       "3  politics/372.txt  Observers to monitor UK election..Ministers wi...   \n",
       "4  politics/333.txt  Kilroy names election seat target..Ex-chat sho...   \n",
       "\n",
       "                                           Summaries  \n",
       "0  - Increase in the stamp duty threshold from £6...  \n",
       "1  \"They are very much not for the good and will ...  \n",
       "2  Michael Howard has denied his shadow cabinet w...  \n",
       "3  The report said individual registration should...  \n",
       "4  UKIP's leader, Roger Knapman, has said he is g...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = os.listdir('../input/bbc-news-summary/BBC News Summary/News Articles')\n",
    "Articles_dir = '../input/bbc-news-summary/BBC News Summary/News Articles/'\n",
    "Summaries_dir = '../input/bbc-news-summary/BBC News Summary/Summaries/'\n",
    "\n",
    "articles = []\n",
    "summaries = []\n",
    "file_arr = []\n",
    "for cls in classes:\n",
    "    files = os.listdir(Articles_dir + cls)\n",
    "    for file in files:\n",
    "        article_file_path = Articles_dir + cls + '/' + file\n",
    "        summary_file_path = Summaries_dir + cls + '/' + file\n",
    "        try:\n",
    "            with open (article_file_path,'r') as f:\n",
    "                articles.append('.'.join([line.rstrip() for line in f.readlines()]))\n",
    "            with open (summary_file_path,'r') as f:\n",
    "                summaries.append('.'.join([line.rstrip() for line in f.readlines()]))\n",
    "            file_arr.append(cls + '/' + file)\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "dataset = pd.DataFrame({'File_path':file_arr,'Articles': articles,'Summaries':summaries})\n",
    "# dataset['Articles'][0]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:51.545256Z",
     "iopub.status.busy": "2022-08-28T10:36:51.544902Z",
     "iopub.status.idle": "2022-08-28T10:36:51.555277Z",
     "shell.execute_reply": "2022-08-28T10:36:51.554355Z",
     "shell.execute_reply.started": "2022-08-28T10:36:51.545221Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:51.558948Z",
     "iopub.status.busy": "2022-08-28T10:36:51.558376Z",
     "iopub.status.idle": "2022-08-28T10:36:51.572279Z",
     "shell.execute_reply": "2022-08-28T10:36:51.571369Z",
     "shell.execute_reply.started": "2022-08-28T10:36:51.558902Z"
    }
   },
   "outputs": [],
   "source": [
    "contractions_dictionary = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
    "\n",
    "                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
    "\n",
    "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "\n",
    "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
    "\n",
    "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
    "\n",
    "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "\n",
    "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
    "\n",
    "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
    "\n",
    "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
    "\n",
    "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
    "\n",
    "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "\n",
    "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
    "\n",
    "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
    "\n",
    "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "\n",
    "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
    "\n",
    "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
    "\n",
    "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
    "\n",
    "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
    "\n",
    "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
    "\n",
    "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "\n",
    "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "\n",
    "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
    "\n",
    "                           \"you're\": \"you are\", \"you've\": \"you have\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:51.574374Z",
     "iopub.status.busy": "2022-08-28T10:36:51.573442Z",
     "iopub.status.idle": "2022-08-28T10:36:52.379416Z",
     "shell.execute_reply": "2022-08-28T10:36:52.378383Z",
     "shell.execute_reply.started": "2022-08-28T10:36:51.574311Z"
    }
   },
   "outputs": [],
   "source": [
    "from string import digits\n",
    "import re\n",
    "\n",
    "def Filter(text):\n",
    "#     pattern = r'[0-9]'\n",
    "\n",
    "# # Match all digits in the string and replace them with an empty string\n",
    "#     text = re.sub(pattern, '', text)\n",
    "#     remove_digits = str.maketrans('', '', digits)\n",
    "#     text = text.translate(remove_digits)\n",
    "    text=text.lower()\n",
    "    text=' '.join([contractions_dictionary[i] if i in contractions_dictionary.keys() else i for i in text.split()])\n",
    "    text=re.sub(r'\\(.*\\)',\"\",text)\n",
    "    text=re.sub(\"'s\",\"\",text)\n",
    "    text=re.sub('\"','',text)\n",
    "    text=' '.join([i for i in text.split() if i.isalpha()])\n",
    "    text=re.sub('[^a-zA-Z]',\" \",text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "dataset['File_path'] = dataset['File_path'].apply(Filter)\n",
    "dataset['Articles'] = dataset['Articles'].apply(Filter)\n",
    "dataset['Summaries'] = dataset['Summaries'].apply(Filter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:52.381738Z",
     "iopub.status.busy": "2022-08-28T10:36:52.380979Z",
     "iopub.status.idle": "2022-08-28T10:36:52.389782Z",
     "shell.execute_reply": "2022-08-28T10:36:52.388720Z",
     "shell.execute_reply.started": "2022-08-28T10:36:52.381698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:52.392067Z",
     "iopub.status.busy": "2022-08-28T10:36:52.391472Z",
     "iopub.status.idle": "2022-08-28T10:36:53.487756Z",
     "shell.execute_reply": "2022-08-28T10:36:53.486711Z",
     "shell.execute_reply.started": "2022-08-28T10:36:52.392027Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "vocab_size = 3000\n",
    "embedding_dim = 100\n",
    "max_length = 100\n",
    "trunc_type='post'\n",
    "padding_type='post'\n",
    "oov_tok = \"<OOV>\"\n",
    "training_size = 2000\n",
    "\n",
    "articles_tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "articles_tokenizer.fit_on_texts(dataset['Articles'])\n",
    "\n",
    "summaries_tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "summaries_tokenizer.fit_on_texts(dataset['Summaries'])\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "training_sequences = articles_tokenizer.texts_to_sequences(dataset['Articles'])\n",
    "training_x_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "\n",
    "training_sequences = summaries_tokenizer.texts_to_sequences(dataset['Summaries'])\n",
    "training_y_padded = pad_sequences(training_sequences, maxlen=30, padding=padding_type, truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:53.491765Z",
     "iopub.status.busy": "2022-08-28T10:36:53.491437Z",
     "iopub.status.idle": "2022-08-28T10:36:53.498604Z",
     "shell.execute_reply": "2022-08-28T10:36:53.497444Z",
     "shell.execute_reply.started": "2022-08-28T10:36:53.491735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_x_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:53.501432Z",
     "iopub.status.busy": "2022-08-28T10:36:53.500719Z",
     "iopub.status.idle": "2022-08-28T10:36:54.498977Z",
     "shell.execute_reply": "2022-08-28T10:36:54.497957Z",
     "shell.execute_reply.started": "2022-08-28T10:36:53.501397Z"
    }
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 2224\n",
    "article_map = dict(map(reversed, articles_tokenizer.word_index.items()))\n",
    "summaries_map = dict(map(reversed, summaries_tokenizer.word_index.items()))\n",
    "\n",
    "article = np.zeros((NUM_TRAIN,100,vocab_size))\n",
    "summ = np.zeros((NUM_TRAIN,30,vocab_size))\n",
    "summ_target = np.zeros((NUM_TRAIN,30,vocab_size))\n",
    "for i,sequence in enumerate(training_x_padded):\n",
    "    for j,word in enumerate(sequence):\n",
    "        article[i,j,word ] = 1\n",
    "        \n",
    "for i,sequence in enumerate(training_y_padded):\n",
    "    for j,word in enumerate(sequence):\n",
    "        summ[i,j,word] = 1\n",
    "        if j>0:\n",
    "            summ_target[i,j-1,word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:54.503846Z",
     "iopub.status.busy": "2022-08-28T10:36:54.503474Z",
     "iopub.status.idle": "2022-08-28T10:36:54.510960Z",
     "shell.execute_reply": "2022-08-28T10:36:54.509993Z",
     "shell.execute_reply.started": "2022-08-28T10:36:54.503819Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2224, 100, 3000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:54.513149Z",
     "iopub.status.busy": "2022-08-28T10:36:54.512165Z",
     "iopub.status.idle": "2022-08-28T10:36:54.521139Z",
     "shell.execute_reply": "2022-08-28T10:36:54.520078Z",
     "shell.execute_reply.started": "2022-08-28T10:36:54.513114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2224, 100, 3000)\n",
      "(2224, 30, 3000)\n",
      "(2224, 30, 3000)\n"
     ]
    }
   ],
   "source": [
    "training_x = article\n",
    "training_y = summ\n",
    "training_y_target = summ_target\n",
    "# testing_x_padded = np.array(testing_x_padded)\n",
    "# testing_y_padded = np.array(testing_y_padded)\n",
    "\n",
    "print(training_x.shape)\n",
    "print(training_y.shape)\n",
    "print(training_y_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:54.523855Z",
     "iopub.status.busy": "2022-08-28T10:36:54.522690Z",
     "iopub.status.idle": "2022-08-28T10:36:57.977784Z",
     "shell.execute_reply": "2022-08-28T10:36:57.976758Z",
     "shell.execute_reply.started": "2022-08-28T10:36:54.523822Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 10:36:54.630027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-28 10:36:54.767263: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-28 10:36:54.768073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-28 10:36:54.769960: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-28 10:36:54.770316: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-28 10:36:54.771161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-28 10:36:54.771841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-28 10:36:57.124389: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-28 10:36:57.125237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-28 10:36:57.125996: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-28 10:36:57.126617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15401 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 3000)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 3000)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 100), (None, 1240400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 100),  1240400     input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 3000)   303000      lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 2,783,800\n",
      "Trainable params: 2,783,800\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(None, vocab_size))\n",
    "encoder = tf.keras.layers.LSTM(embedding_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None, vocab_size))\n",
    "decoder_lstm = tf.keras.layers.LSTM(embedding_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:36:57.979584Z",
     "iopub.status.busy": "2022-08-28T10:36:57.979229Z",
     "iopub.status.idle": "2022-08-28T10:52:40.458285Z",
     "shell.execute_reply": "2022-08-28T10:52:40.457265Z",
     "shell.execute_reply.started": "2022-08-28T10:36:57.979548Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 10:37:00.031575: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2668800000 exceeds 10% of free system memory.\n",
      "2022-08-28 10:37:03.692780: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 800640000 exceeds 10% of free system memory.\n",
      "2022-08-28 10:37:05.178368: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 800640000 exceeds 10% of free system memory.\n",
      "2022-08-28 10:37:06.083979: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 2668800000 exceeds 10% of free system memory.\n",
      "2022-08-28 10:37:08.212757: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 800640000 exceeds 10% of free system memory.\n",
      "2022-08-28 10:37:09.632301: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-28 10:37:13.238912: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 8s 87ms/step - loss: 6.9803 - accuracy: 0.0990\n",
      "Epoch 2/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 5.7275 - accuracy: 0.1084\n",
      "Epoch 3/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 5.6975 - accuracy: 0.1084\n",
      "Epoch 4/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 5.6941 - accuracy: 0.1084\n",
      "Epoch 5/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 5.6927 - accuracy: 0.1084\n",
      "Epoch 6/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 5.6911 - accuracy: 0.1084\n",
      "Epoch 7/300\n",
      "35/35 [==============================] - 4s 100ms/step - loss: 5.6888 - accuracy: 0.1084\n",
      "Epoch 8/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 5.6840 - accuracy: 0.1084\n",
      "Epoch 9/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 5.6779 - accuracy: 0.1084\n",
      "Epoch 10/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 5.6721 - accuracy: 0.1084\n",
      "Epoch 11/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 5.6649 - accuracy: 0.1084\n",
      "Epoch 12/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 5.6587 - accuracy: 0.1084\n",
      "Epoch 13/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 5.6501 - accuracy: 0.1084\n",
      "Epoch 14/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 5.6414 - accuracy: 0.1084\n",
      "Epoch 15/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 5.6321 - accuracy: 0.1084\n",
      "Epoch 16/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 5.6217 - accuracy: 0.1084\n",
      "Epoch 17/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 5.6088 - accuracy: 0.1095\n",
      "Epoch 18/300\n",
      "35/35 [==============================] - 3s 96ms/step - loss: 5.5963 - accuracy: 0.1124\n",
      "Epoch 19/300\n",
      "35/35 [==============================] - 3s 96ms/step - loss: 5.5815 - accuracy: 0.1174\n",
      "Epoch 20/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 5.5648 - accuracy: 0.1231\n",
      "Epoch 21/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 5.5466 - accuracy: 0.1252\n",
      "Epoch 22/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 5.5278 - accuracy: 0.1275\n",
      "Epoch 23/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 5.5071 - accuracy: 0.1289\n",
      "Epoch 24/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 5.4869 - accuracy: 0.1299\n",
      "Epoch 25/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 5.4651 - accuracy: 0.1313\n",
      "Epoch 26/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 5.4428 - accuracy: 0.1332\n",
      "Epoch 27/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 5.4207 - accuracy: 0.1369\n",
      "Epoch 28/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 5.3964 - accuracy: 0.1400\n",
      "Epoch 29/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 5.3717 - accuracy: 0.1438\n",
      "Epoch 30/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 5.3464 - accuracy: 0.1478\n",
      "Epoch 31/300\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 5.3223 - accuracy: 0.1512\n",
      "Epoch 32/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 5.2958 - accuracy: 0.1547\n",
      "Epoch 33/300\n",
      "35/35 [==============================] - 4s 101ms/step - loss: 5.2714 - accuracy: 0.1577\n",
      "Epoch 34/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 5.2483 - accuracy: 0.1604\n",
      "Epoch 35/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 5.2230 - accuracy: 0.1630\n",
      "Epoch 36/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 5.1994 - accuracy: 0.1649\n",
      "Epoch 37/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 5.1758 - accuracy: 0.1681\n",
      "Epoch 38/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 5.1532 - accuracy: 0.1710\n",
      "Epoch 39/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 5.1315 - accuracy: 0.1731\n",
      "Epoch 40/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 5.1167 - accuracy: 0.1735\n",
      "Epoch 41/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 5.0908 - accuracy: 0.1753\n",
      "Epoch 42/300\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 5.0699 - accuracy: 0.1760\n",
      "Epoch 43/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 5.0508 - accuracy: 0.1776\n",
      "Epoch 44/300\n",
      "35/35 [==============================] - 4s 103ms/step - loss: 5.0312 - accuracy: 0.1787\n",
      "Epoch 45/300\n",
      "35/35 [==============================] - 3s 100ms/step - loss: 5.0116 - accuracy: 0.1807\n",
      "Epoch 46/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 4.9932 - accuracy: 0.1815\n",
      "Epoch 47/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 4.9750 - accuracy: 0.1827\n",
      "Epoch 48/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 4.9572 - accuracy: 0.1839\n",
      "Epoch 49/300\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 4.9407 - accuracy: 0.1849\n",
      "Epoch 50/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 4.9254 - accuracy: 0.1863\n",
      "Epoch 51/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.9065 - accuracy: 0.1881\n",
      "Epoch 52/300\n",
      "35/35 [==============================] - 3s 96ms/step - loss: 4.8907 - accuracy: 0.1899\n",
      "Epoch 53/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 4.8745 - accuracy: 0.1909\n",
      "Epoch 54/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.8585 - accuracy: 0.1927\n",
      "Epoch 55/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 4.8433 - accuracy: 0.1939\n",
      "Epoch 56/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 4.8284 - accuracy: 0.1950\n",
      "Epoch 57/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 4.8148 - accuracy: 0.1962\n",
      "Epoch 58/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 4.7980 - accuracy: 0.1984\n",
      "Epoch 59/300\n",
      "35/35 [==============================] - 4s 107ms/step - loss: 4.7825 - accuracy: 0.1986\n",
      "Epoch 60/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 4.7677 - accuracy: 0.2000\n",
      "Epoch 61/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 4.7522 - accuracy: 0.2021\n",
      "Epoch 62/300\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 4.7374 - accuracy: 0.2033\n",
      "Epoch 63/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 4.7240 - accuracy: 0.2046\n",
      "Epoch 64/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.7081 - accuracy: 0.2059\n",
      "Epoch 65/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 4.6940 - accuracy: 0.2077\n",
      "Epoch 66/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 4.6793 - accuracy: 0.2098\n",
      "Epoch 67/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.6647 - accuracy: 0.2108\n",
      "Epoch 68/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 4.6511 - accuracy: 0.2123\n",
      "Epoch 69/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.6348 - accuracy: 0.2137\n",
      "Epoch 70/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 4.6243 - accuracy: 0.2150\n",
      "Epoch 71/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 4.6075 - accuracy: 0.2162\n",
      "Epoch 72/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 4.5942 - accuracy: 0.2177\n",
      "Epoch 73/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 4.5785 - accuracy: 0.2196\n",
      "Epoch 74/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 4.5664 - accuracy: 0.2209\n",
      "Epoch 75/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 4.5510 - accuracy: 0.2227\n",
      "Epoch 76/300\n",
      "35/35 [==============================] - 4s 106ms/step - loss: 4.5376 - accuracy: 0.2238\n",
      "Epoch 77/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 4.5234 - accuracy: 0.2248\n",
      "Epoch 78/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.5133 - accuracy: 0.2256\n",
      "Epoch 79/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 4.4985 - accuracy: 0.2275\n",
      "Epoch 80/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 4.4830 - accuracy: 0.2288\n",
      "Epoch 81/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 4.4691 - accuracy: 0.2307\n",
      "Epoch 82/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 4.4556 - accuracy: 0.2318\n",
      "Epoch 83/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.4432 - accuracy: 0.2331\n",
      "Epoch 84/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 4.4290 - accuracy: 0.2345\n",
      "Epoch 85/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.4157 - accuracy: 0.2359\n",
      "Epoch 86/300\n",
      "35/35 [==============================] - 3s 99ms/step - loss: 4.4027 - accuracy: 0.2366\n",
      "Epoch 87/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 4.3871 - accuracy: 0.2399\n",
      "Epoch 88/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 4.3712 - accuracy: 0.2404\n",
      "Epoch 89/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 4.3594 - accuracy: 0.2417\n",
      "Epoch 90/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 4.3469 - accuracy: 0.2422\n",
      "Epoch 91/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 4.3315 - accuracy: 0.2443\n",
      "Epoch 92/300\n",
      "35/35 [==============================] - 3s 81ms/step - loss: 4.3179 - accuracy: 0.2450\n",
      "Epoch 93/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 4.3056 - accuracy: 0.2462\n",
      "Epoch 94/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 4.2916 - accuracy: 0.2478\n",
      "Epoch 95/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 4.2781 - accuracy: 0.2495\n",
      "Epoch 96/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 4.2646 - accuracy: 0.2505\n",
      "Epoch 97/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 4.2502 - accuracy: 0.2522\n",
      "Epoch 98/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 4.2396 - accuracy: 0.2530\n",
      "Epoch 99/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 4.2250 - accuracy: 0.2548\n",
      "Epoch 100/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.2101 - accuracy: 0.2567\n",
      "Epoch 101/300\n",
      "35/35 [==============================] - 4s 107ms/step - loss: 4.1978 - accuracy: 0.2576\n",
      "Epoch 102/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 4.1824 - accuracy: 0.2600\n",
      "Epoch 103/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 4.1690 - accuracy: 0.2597\n",
      "Epoch 104/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 4.1559 - accuracy: 0.2624\n",
      "Epoch 105/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 4.1452 - accuracy: 0.2630\n",
      "Epoch 106/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 4.1319 - accuracy: 0.2651\n",
      "Epoch 107/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 4.1184 - accuracy: 0.2661\n",
      "Epoch 108/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 4.1059 - accuracy: 0.2674\n",
      "Epoch 109/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 4.0944 - accuracy: 0.2684\n",
      "Epoch 110/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 4.0836 - accuracy: 0.2691\n",
      "Epoch 111/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 4.0668 - accuracy: 0.2713\n",
      "Epoch 112/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 4.0560 - accuracy: 0.2720\n",
      "Epoch 113/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 4.0432 - accuracy: 0.2745\n",
      "Epoch 114/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 4.0322 - accuracy: 0.2751\n",
      "Epoch 115/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 4.0184 - accuracy: 0.2773\n",
      "Epoch 116/300\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 4.0028 - accuracy: 0.2791\n",
      "Epoch 117/300\n",
      "35/35 [==============================] - 3s 97ms/step - loss: 3.9901 - accuracy: 0.2797\n",
      "Epoch 118/300\n",
      "35/35 [==============================] - 3s 81ms/step - loss: 3.9798 - accuracy: 0.2804\n",
      "Epoch 119/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.9716 - accuracy: 0.2806\n",
      "Epoch 120/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 3.9567 - accuracy: 0.2829\n",
      "Epoch 121/300\n",
      "35/35 [==============================] - 3s 80ms/step - loss: 3.9438 - accuracy: 0.2843\n",
      "Epoch 122/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.9312 - accuracy: 0.2859\n",
      "Epoch 123/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 3.9191 - accuracy: 0.2864\n",
      "Epoch 124/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 3.9048 - accuracy: 0.2878\n",
      "Epoch 125/300\n",
      "35/35 [==============================] - 3s 80ms/step - loss: 3.8940 - accuracy: 0.2895\n",
      "Epoch 126/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.8831 - accuracy: 0.2911\n",
      "Epoch 127/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 3.8781 - accuracy: 0.2907\n",
      "Epoch 128/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 3.8655 - accuracy: 0.2931\n",
      "Epoch 129/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 3.8495 - accuracy: 0.2934\n",
      "Epoch 130/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 3.8372 - accuracy: 0.2960\n",
      "Epoch 131/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 3.8286 - accuracy: 0.2959\n",
      "Epoch 132/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 3.8126 - accuracy: 0.2984\n",
      "Epoch 133/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.8062 - accuracy: 0.2997\n",
      "Epoch 134/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 3.7997 - accuracy: 0.2985\n",
      "Epoch 135/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 3.7838 - accuracy: 0.3009\n",
      "Epoch 136/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.7677 - accuracy: 0.3035\n",
      "Epoch 137/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 3.7563 - accuracy: 0.3047\n",
      "Epoch 138/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 3.7441 - accuracy: 0.3061\n",
      "Epoch 139/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.7369 - accuracy: 0.3073\n",
      "Epoch 140/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.7241 - accuracy: 0.3085\n",
      "Epoch 141/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 3.7111 - accuracy: 0.3099\n",
      "Epoch 142/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.7016 - accuracy: 0.3116\n",
      "Epoch 143/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.6910 - accuracy: 0.3131\n",
      "Epoch 144/300\n",
      "35/35 [==============================] - 3s 97ms/step - loss: 3.6785 - accuracy: 0.3136\n",
      "Epoch 145/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 3.6691 - accuracy: 0.3150\n",
      "Epoch 146/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 3.6574 - accuracy: 0.3155\n",
      "Epoch 147/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.6459 - accuracy: 0.3176\n",
      "Epoch 148/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 3.6382 - accuracy: 0.3183\n",
      "Epoch 149/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 3.6280 - accuracy: 0.3199\n",
      "Epoch 150/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.6181 - accuracy: 0.3206\n",
      "Epoch 151/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.6076 - accuracy: 0.3211\n",
      "Epoch 152/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 3.6033 - accuracy: 0.3222\n",
      "Epoch 153/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 3.5855 - accuracy: 0.3250\n",
      "Epoch 154/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 3.5768 - accuracy: 0.3256\n",
      "Epoch 155/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.5622 - accuracy: 0.3282\n",
      "Epoch 156/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 3.5497 - accuracy: 0.3293\n",
      "Epoch 157/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.5395 - accuracy: 0.3303\n",
      "Epoch 158/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 3.5303 - accuracy: 0.3319\n",
      "Epoch 159/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 3.5195 - accuracy: 0.3331\n",
      "Epoch 160/300\n",
      "35/35 [==============================] - 4s 101ms/step - loss: 3.5145 - accuracy: 0.3325\n",
      "Epoch 161/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.5022 - accuracy: 0.3354\n",
      "Epoch 162/300\n",
      "35/35 [==============================] - 3s 81ms/step - loss: 3.4897 - accuracy: 0.3367\n",
      "Epoch 163/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 3.4758 - accuracy: 0.3388\n",
      "Epoch 164/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 3.4668 - accuracy: 0.3408\n",
      "Epoch 165/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 3.4600 - accuracy: 0.3402\n",
      "Epoch 166/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 3.4547 - accuracy: 0.3408\n",
      "Epoch 167/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 3.4537 - accuracy: 0.3406\n",
      "Epoch 168/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.4405 - accuracy: 0.3426\n",
      "Epoch 169/300\n",
      "35/35 [==============================] - 3s 81ms/step - loss: 3.4262 - accuracy: 0.3444\n",
      "Epoch 170/300\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 3.4107 - accuracy: 0.3471\n",
      "Epoch 171/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 3.4006 - accuracy: 0.3490\n",
      "Epoch 172/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.3912 - accuracy: 0.3492\n",
      "Epoch 173/300\n",
      "35/35 [==============================] - 3s 81ms/step - loss: 3.3873 - accuracy: 0.3502\n",
      "Epoch 174/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 3.3721 - accuracy: 0.3530\n",
      "Epoch 175/300\n",
      "35/35 [==============================] - 3s 99ms/step - loss: 3.3692 - accuracy: 0.3520\n",
      "Epoch 176/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 3.3572 - accuracy: 0.3547\n",
      "Epoch 177/300\n",
      "35/35 [==============================] - 3s 95ms/step - loss: 3.3476 - accuracy: 0.3544\n",
      "Epoch 178/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 3.3416 - accuracy: 0.3557\n",
      "Epoch 179/300\n",
      "35/35 [==============================] - 3s 80ms/step - loss: 3.3316 - accuracy: 0.3577\n",
      "Epoch 180/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 3.3207 - accuracy: 0.3589\n",
      "Epoch 181/300\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 3.3171 - accuracy: 0.3594\n",
      "Epoch 182/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.3052 - accuracy: 0.3609\n",
      "Epoch 183/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.2938 - accuracy: 0.3628\n",
      "Epoch 184/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 3.2799 - accuracy: 0.3655\n",
      "Epoch 185/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 3.2708 - accuracy: 0.3667\n",
      "Epoch 186/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 3.2655 - accuracy: 0.3668\n",
      "Epoch 187/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.2528 - accuracy: 0.3688\n",
      "Epoch 188/300\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 3.2426 - accuracy: 0.3707\n",
      "Epoch 189/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 3.2319 - accuracy: 0.3723\n",
      "Epoch 190/300\n",
      "35/35 [==============================] - 3s 100ms/step - loss: 3.2257 - accuracy: 0.3725\n",
      "Epoch 191/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 3.2164 - accuracy: 0.3741\n",
      "Epoch 192/300\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 3.2143 - accuracy: 0.3750\n",
      "Epoch 193/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.2079 - accuracy: 0.3756\n",
      "Epoch 194/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.1940 - accuracy: 0.3772\n",
      "Epoch 195/300\n",
      "35/35 [==============================] - 3s 96ms/step - loss: 3.1835 - accuracy: 0.3777\n",
      "Epoch 196/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.1799 - accuracy: 0.3796\n",
      "Epoch 197/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.1734 - accuracy: 0.3796\n",
      "Epoch 198/300\n",
      "35/35 [==============================] - 3s 81ms/step - loss: 3.1681 - accuracy: 0.3803\n",
      "Epoch 199/300\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 3.1559 - accuracy: 0.3824\n",
      "Epoch 200/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.1433 - accuracy: 0.3850\n",
      "Epoch 201/300\n",
      "35/35 [==============================] - 3s 100ms/step - loss: 3.1325 - accuracy: 0.3872\n",
      "Epoch 202/300\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 3.1295 - accuracy: 0.3874\n",
      "Epoch 203/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.1193 - accuracy: 0.3888\n",
      "Epoch 204/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.1057 - accuracy: 0.3917\n",
      "Epoch 205/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 3.0966 - accuracy: 0.3919\n",
      "Epoch 206/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 3.1182 - accuracy: 0.3871\n",
      "Epoch 207/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.0965 - accuracy: 0.3908\n",
      "Epoch 208/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.0797 - accuracy: 0.3940\n",
      "Epoch 209/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 3.0722 - accuracy: 0.3960\n",
      "Epoch 210/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 3.0596 - accuracy: 0.3982\n",
      "Epoch 211/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.0489 - accuracy: 0.3998\n",
      "Epoch 212/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.0431 - accuracy: 0.4010\n",
      "Epoch 213/300\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 3.0367 - accuracy: 0.4018\n",
      "Epoch 214/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 3.0296 - accuracy: 0.4028\n",
      "Epoch 215/300\n",
      "35/35 [==============================] - 3s 99ms/step - loss: 3.0239 - accuracy: 0.4030\n",
      "Epoch 216/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 3.0103 - accuracy: 0.4060\n",
      "Epoch 217/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 3.0001 - accuracy: 0.4079\n",
      "Epoch 218/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.9905 - accuracy: 0.4105\n",
      "Epoch 219/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.9882 - accuracy: 0.4080\n",
      "Epoch 220/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 2.9808 - accuracy: 0.4113\n",
      "Epoch 221/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.9738 - accuracy: 0.4112\n",
      "Epoch 222/300\n",
      "35/35 [==============================] - 3s 80ms/step - loss: 2.9688 - accuracy: 0.4119\n",
      "Epoch 223/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 2.9775 - accuracy: 0.4105\n",
      "Epoch 224/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 2.9692 - accuracy: 0.4120\n",
      "Epoch 225/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.9517 - accuracy: 0.4148\n",
      "Epoch 226/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.9358 - accuracy: 0.4175\n",
      "Epoch 227/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 2.9225 - accuracy: 0.4202\n",
      "Epoch 228/300\n",
      "35/35 [==============================] - 3s 100ms/step - loss: 2.9221 - accuracy: 0.4187\n",
      "Epoch 229/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 2.9096 - accuracy: 0.4222\n",
      "Epoch 230/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.8996 - accuracy: 0.4233\n",
      "Epoch 231/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 2.8909 - accuracy: 0.4246\n",
      "Epoch 232/300\n",
      "35/35 [==============================] - 3s 81ms/step - loss: 2.8932 - accuracy: 0.4237\n",
      "Epoch 233/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 2.8867 - accuracy: 0.4256\n",
      "Epoch 234/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 2.8781 - accuracy: 0.4269\n",
      "Epoch 235/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 2.8646 - accuracy: 0.4291\n",
      "Epoch 236/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.8570 - accuracy: 0.4301\n",
      "Epoch 237/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.8512 - accuracy: 0.4314\n",
      "Epoch 238/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 2.8471 - accuracy: 0.4325\n",
      "Epoch 239/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.8365 - accuracy: 0.4334\n",
      "Epoch 240/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.8343 - accuracy: 0.4335\n",
      "Epoch 241/300\n",
      "35/35 [==============================] - 3s 99ms/step - loss: 2.8267 - accuracy: 0.4346\n",
      "Epoch 242/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 2.8281 - accuracy: 0.4347\n",
      "Epoch 243/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.8136 - accuracy: 0.4372\n",
      "Epoch 244/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 2.8044 - accuracy: 0.4396\n",
      "Epoch 245/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 2.7973 - accuracy: 0.4399\n",
      "Epoch 246/300\n",
      "35/35 [==============================] - 3s 88ms/step - loss: 2.7895 - accuracy: 0.4409\n",
      "Epoch 247/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 2.7774 - accuracy: 0.4436\n",
      "Epoch 248/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 2.7747 - accuracy: 0.4437\n",
      "Epoch 249/300\n",
      "35/35 [==============================] - 3s 94ms/step - loss: 2.7690 - accuracy: 0.4445\n",
      "Epoch 250/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 2.7608 - accuracy: 0.4457\n",
      "Epoch 251/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.7498 - accuracy: 0.4488\n",
      "Epoch 252/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.7419 - accuracy: 0.4500\n",
      "Epoch 253/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 2.7329 - accuracy: 0.4511\n",
      "Epoch 254/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 2.7254 - accuracy: 0.4528\n",
      "Epoch 255/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 2.7208 - accuracy: 0.4531\n",
      "Epoch 256/300\n",
      "35/35 [==============================] - 4s 103ms/step - loss: 2.7212 - accuracy: 0.4536\n",
      "Epoch 257/300\n",
      "35/35 [==============================] - 3s 81ms/step - loss: 2.7156 - accuracy: 0.4537\n",
      "Epoch 258/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 2.7054 - accuracy: 0.4565\n",
      "Epoch 259/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 2.7048 - accuracy: 0.4556\n",
      "Epoch 260/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 2.6970 - accuracy: 0.4558\n",
      "Epoch 261/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 2.6891 - accuracy: 0.4587\n",
      "Epoch 262/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.6733 - accuracy: 0.4616\n",
      "Epoch 263/300\n",
      "35/35 [==============================] - 3s 90ms/step - loss: 2.6660 - accuracy: 0.4634\n",
      "Epoch 264/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.6760 - accuracy: 0.4595\n",
      "Epoch 265/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.6563 - accuracy: 0.4651\n",
      "Epoch 266/300\n",
      "35/35 [==============================] - 3s 99ms/step - loss: 2.6469 - accuracy: 0.4661\n",
      "Epoch 267/300\n",
      "35/35 [==============================] - 3s 92ms/step - loss: 2.6381 - accuracy: 0.4676\n",
      "Epoch 268/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.6320 - accuracy: 0.4688\n",
      "Epoch 269/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.6253 - accuracy: 0.4693\n",
      "Epoch 270/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 2.6203 - accuracy: 0.4703\n",
      "Epoch 271/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 2.6181 - accuracy: 0.4700\n",
      "Epoch 272/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 2.6141 - accuracy: 0.4705\n",
      "Epoch 273/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.6053 - accuracy: 0.4725\n",
      "Epoch 274/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 2.5972 - accuracy: 0.4740\n",
      "Epoch 275/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 2.5871 - accuracy: 0.4761\n",
      "Epoch 276/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 2.5836 - accuracy: 0.4766\n",
      "Epoch 277/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.5843 - accuracy: 0.4768\n",
      "Epoch 278/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 2.5732 - accuracy: 0.4787\n",
      "Epoch 279/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.5647 - accuracy: 0.4792\n",
      "Epoch 280/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.5546 - accuracy: 0.4817\n",
      "Epoch 281/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 2.5449 - accuracy: 0.4830\n",
      "Epoch 282/300\n",
      "35/35 [==============================] - 4s 105ms/step - loss: 2.5416 - accuracy: 0.4830\n",
      "Epoch 283/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.5532 - accuracy: 0.4800\n",
      "Epoch 284/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.5406 - accuracy: 0.4832\n",
      "Epoch 285/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 2.5299 - accuracy: 0.4848\n",
      "Epoch 286/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 2.5263 - accuracy: 0.4855\n",
      "Epoch 287/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 2.5252 - accuracy: 0.4862\n",
      "Epoch 288/300\n",
      "35/35 [==============================] - 3s 89ms/step - loss: 2.5139 - accuracy: 0.4871\n",
      "Epoch 289/300\n",
      "35/35 [==============================] - 3s 87ms/step - loss: 2.5078 - accuracy: 0.4894\n",
      "Epoch 290/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 2.4921 - accuracy: 0.4920\n",
      "Epoch 291/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.4895 - accuracy: 0.4941\n",
      "Epoch 292/300\n",
      "35/35 [==============================] - 3s 93ms/step - loss: 2.4748 - accuracy: 0.4958\n",
      "Epoch 293/300\n",
      "35/35 [==============================] - 3s 83ms/step - loss: 2.4679 - accuracy: 0.4970\n",
      "Epoch 294/300\n",
      "35/35 [==============================] - 3s 86ms/step - loss: 2.4606 - accuracy: 0.4979\n",
      "Epoch 295/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 2.4534 - accuracy: 0.4994\n",
      "Epoch 296/300\n",
      "35/35 [==============================] - 4s 107ms/step - loss: 2.4550 - accuracy: 0.4989\n",
      "Epoch 297/300\n",
      "35/35 [==============================] - 3s 85ms/step - loss: 2.4436 - accuracy: 0.5003\n",
      "Epoch 298/300\n",
      "35/35 [==============================] - 3s 84ms/step - loss: 2.4316 - accuracy: 0.5042\n",
      "Epoch 299/300\n",
      "35/35 [==============================] - 3s 91ms/step - loss: 2.4274 - accuracy: 0.5035\n",
      "Epoch 300/300\n",
      "35/35 [==============================] - 3s 82ms/step - loss: 2.4362 - accuracy: 0.5011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7684043710>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([training_x,training_y],training_y_target,\n",
    "          epochs=300,\n",
    "          batch_size=64,\n",
    "          callbacks = [tf.keras.callbacks.CSVLogger('./training.csv')]\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:52:58.167646Z",
     "iopub.status.busy": "2022-08-28T10:52:58.167075Z",
     "iopub.status.idle": "2022-08-28T10:52:58.721586Z",
     "shell.execute_reply": "2022-08-28T10:52:58.720631Z",
     "shell.execute_reply.started": "2022-08-28T10:52:58.167602Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "model.save_weights('./my_checkpoint')\n",
    "\n",
    "encoder_model = tf.keras.Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape = (embedding_dim,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape = (embedding_dim,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = tf.keras.Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:52:58.725818Z",
     "iopub.status.busy": "2022-08-28T10:52:58.725249Z",
     "iopub.status.idle": "2022-08-28T10:52:59.120086Z",
     "shell.execute_reply": "2022-08-28T10:52:59.119179Z",
     "shell.execute_reply.started": "2022-08-28T10:52:58.725790Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualising the Accuracy and loss plots\n",
    "\n",
    "callback_csv = pd.read_csv('./training.csv')\n",
    "acc = callback_csv['accuracy']\n",
    "loss = callback_csv['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:52:59.387640Z",
     "iopub.status.busy": "2022-08-28T10:52:59.386941Z",
     "iopub.status.idle": "2022-08-28T10:52:59.398768Z",
     "shell.execute_reply": "2022-08-28T10:52:59.397787Z",
     "shell.execute_reply.started": "2022-08-28T10:52:59.387596Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define Decode Sequence\n",
    "def decode_sequence(input_seq):\n",
    "    #Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    #Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, 3000))\n",
    "    #Get the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, summaries_tokenizer.word_index['<OOV>']] = 1.\n",
    "\n",
    "    #Sampling loop for a batch of sequences\n",
    "    #(to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        #Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = summaries_map[sampled_token_index]\n",
    "        decoded_sentence += ' '+ sampled_char\n",
    "        \n",
    "        #Exit condition: either hit max length\n",
    "        #or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > 70):\n",
    "            stop_condition = True\n",
    "\n",
    "        #Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, 3000))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        #Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:52:59.955421Z",
     "iopub.status.busy": "2022-08-28T10:52:59.954306Z",
     "iopub.status.idle": "2022-08-28T10:53:07.117662Z",
     "shell.execute_reply": "2022-08-28T10:53:07.116610Z",
     "shell.execute_reply.started": "2022-08-28T10:52:59.955375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: budget to set scene for brown will seek to put the economy at the centre of labour bid for a third term in power when he delivers his ninth budget at he is expected to stress the importance of continued economic with low unemployment and interest the chancellor is expected to freeze petrol duty and raise the stamp duty threshold from but the conservatives and lib dems insist voters face higher taxes and more under officials have said there will not be a but mr brown is thought to have about to increase in the stamp duty threshold from a freeze on petrol an extension of tax credit scheme for poorer possible help for pensioners the stamp duty threshold rise is intended to help first time buyers a likely theme of all three of the main general election ten years buyers had a much greater chance of avoiding stamp with close to half a million in england and wales selling for less than since average uk property prices have more than doubled while the starting threshold for stamp duty has not tax credits as a the number of properties incurring stamp duty has rocketed as has the government tax the liberal democrats unveiled their own proposals to raise the stamp duty threshold to in tories are also thought likely to propose increased with shadow chancellor oliver letwin branding stamp duty a classic labour stealth the tories say whatever the chancellor gives away will be clawed back in higher taxes if labour is returned to shadow treasury chief secretary george osborne everyone who looks at the british economy at the moment says there has been a sharp deterioration in the public that there is a black he if labour is elected there will be a very substantial tax increase in the budget after the of the order of around mr brown former advisor ed now a parliamentary said an examination of tory plans for the economy showed there would be a difference in investment by the end of the next parliament between the two main he i do not accept there is any need for any changes to the plans we have set out to meet our spending the lib dems david laws the chancellor will no doubt tell us today how wonderfully the economy is he but a lot of that is built on an increase in personal and consumer debt over the last few years that makes the economy quite vulnerable potentially if interest rates ever do have to go up in a significant snp leader alex salmond said his party would introduce a grant for first time reduce corporation tax and introduce a citizens pension free from means plaid cymru economics spokesman adam price said he wanted help to get people on the housing ladder and an increase in the minimum wage to an\n",
      "Decoded sentence:  says the us trade <OOV> has <OOV> in january year <OOV> of the best male\n",
      "-\n",
      "Input sentence: army chiefs in regiments chiefs are expected to meet to make a final decision on the future of scotland army committee of the army which is made up of the most senior defence will discuss plans for restructuring regiments on the proposals include cutting scotland six regiments to five and merging these into a super the plans have faced stiff opposition from campaigners and politicians the committee decision must be ratified by defence secretary geoff hoon and prime minister tony it is expected that it will be made public next when ministers announced a reorganisation of the army it drew a question mark over the futures of the black the kings own scottish the royal the royal highland fusiliers and the argyll and sutherland in the council of scottish colonels proposed the merger of the royal scots and the king own scottish borderers into a single their it would be one of five in the new super the proposals to either merge or amalgamate the six regiments into a super regiment sparked a political with labour backbenchers and opposition politicians opposing the they felt the timing was insensitive because the black watch was in the frontline in suffering the save the scottish regiments campaigners were so angered they threatened to stand against labour at the next general ahead of the expected army board a spokesman the government and the army board have spent the past four months attempting to trick serving soldiers and the public into thinking their planned changes for the scottish regiments are for the good of the army and for that of the serving they are very much not for the good and will destroy scotland regiments by moulding them into a single super regiment which will lead to severe recruitment a loss of local connections to those regiments and a loss to scotland of an important part of her heritage most her future the regiments are the envy of armies around the an alternative blueprint had been put forward by labour mp eric who proposed going ahead with the merger while preserving the other for a brief there was speculation the prime minister might consider the but that now seems speaking in scotland last mr blair said the aim was to preserve tradition but introduce a more effective structure and hinted that a super regiment was he they do not want to get rid of the history or the traditions of the regiment or the local connections far from all they want to do is make sure they can transfer people easily across regiments and deploy them more the prime minister said he hoped concerns would be taken into account but the need for effective change had to be\n",
      "Decoded sentence:  has filed <OOV> and leicester as star of <OOV> state last year <OOV> said\n",
      "-\n",
      "Input sentence: howard denies split over id howard has denied his shadow cabinet was split over its decision to back controversial labour plans to introduce id tory leader said his front bench team had reached a collective view after holding a good but admitted it was not an easy he had decided to support the plans as the police said they would help fight crime and illegal the lib dems have pledged to oppose the bill when it is debated next sources say senior party figures had argued vociferously against the id card among those reported to have serious reservations over the strategy were senior shadow cabinet members david oliver letwin and tim but mr howard denied mr his transport and environment said the plans he also said he was confident shadow home secretary mr davis would set out the position very clearly when he stands up to debate the matter next mr howard said the police had said id cards could help them foil a terror bomb plot in which people could lose their he when the police say that you have to take them acknowledged there were good libertarian arguments against the but said the shadow cabinet had weighed up all the conflicting interests before reaching its i do not pretend that it is an easy decision but at the end of the day a decision has to be he also denied he was afraid of looking soft on the compared to the conservatives announced their support for the government plans on monday within the party told the bbc mr howard had always been in favour of id and tried to introduce them when he was home but the tories insisted they would hold ministers to account over the precise purpose of the said they would also press labour over whether objectives could be met and whether the home office would be able to deliver and they pledged to assess the cost effectiveness of id cards and whether people privacy would be properly it is important to remember that this bill will take a decade to come into full a spokesman lib dem home affairs spokesman mark oaten has branded the id scheme a waste of money and deeply he this has all the signs of michael howard overruling concerns over id chairman of the bar guy mansfield qc warned there was a real risk that people on the margins of society would be driven into the hands of what is going to happen to young asian men when there has been a bomb gone off they are going to be if they have not they are going to be tory douglas hogg said he opposed the plans for id cards branding them a regressive step which would intrude into the lives of ordinary citizens without any counterbalancing he predicted ultimately carrying the cards would become compulsory and that would lead to large numbers of britain ethnic minorities being stopped by\n",
      "Decoded sentence:  has been mp where the <OOV> broadcasting corporation was surprised he said\n",
      "-\n",
      "Input sentence: observers to monitor uk will invite international observers to check the forthcoming uk general election is fairly move comes amid claims the poll could be marred by electoral a report by two mps committees called on thursday for urgent changes to the electoral registration system to combat vote rigging and boost but in a written response to labour mp gordon the government said it would normally invite observers to any uk constitutional affairs minister christopher leslie i fully expect us to repeat our previous practice of doing so once the date for the next general election is the government has looked at ways of boosting voter which fell to in the last general election in trial ballots in four english regions last summer were hit by delays and some fraud liberal democrat peer lord greaves called last week for international observers at the general election saying otherwise there could be months of court challenges on a scale not seen since the thursday report was drawn up by two committees scrutinising the work of the office of the deputy prime minister it said with the growth of postal there was a strong case to tighten up fraud protection by requiring voters to register rather than by it also said about three million people eligible to vote are not registered to do for the general election suggest of people aged between and and of black voters were not on the electoral young people in shared accommodation are thought to miss out because no one acts as head of the household to fill in the odpm committee chairman andrew bennett said individual voter as opposed to registration by should be quickly introduced as it could dramatically reduce the chances of but his counterpart on the dca alan said it should be delayed until measures likely to increase registration have been put in place and proved shadow constitutional affairs secretary oliver heald accused the government of dragging its feet over this badly needed it is vital that we move ahead with the northern ireland system of individual electoral registration to safeguard the integrity of the britain electoral he report said individual registration should be treated carefully as of voters disappeared from the electoral roll in northern ireland when it was introduced in the report said the government should consider fines for unregistered but accepted many experts said it would be an expensive system that would be hard to it said incentives to such as council tax were likely to be seen as gimmicks and risked undermining the integrity of the mps instead they called for imaginative campaigns to boost\n",
      "Decoded sentence:  said it is <OOV> to be the <OOV> of its film version of the <OOV> <OOV>\n",
      "-\n",
      "Input sentence: kilroy names election seat show host robert is to contest the derbyshire seat of erewash at the next general elizabeth blackman won the seat in and has a she says she will fight on her record as a constituency mr announced his plans a day after launching his new the latin for the east midlands who quit the uk independence wants his new group to change the face of uk his choice of election constituency quashes speculation that he would stand against defence secretary geoff hoon in ukip won of the vote in erewash in last june european elections with mr among their candidates for the until erewash had been held by the tories since ms blackman said she was proud of the government achievements in the she declined to give her view of mr at this he told a london news conference that veritas would avoid the old lies and he said our country was being stolen from us by mass immigration and promised a firm but fair policy on veritas says it hopes to contest most seats at the forthcoming general election but plans to announce detailed policies on health and defence over the next few leader roger knapman says he is glad to see the back of mr labour campaign spokesman fraser kemp said veritas was joining an already crowded field on the right of british mr was joined in the new venture by one of ukip two london assembly damien who is now deputy chairman petrina holdsworth has said the group will just be a parody of the party the men have mr quit ukip last week after months of tension as he vied unsuccessfully for the leadership of that he said he was ashamed to be a member of a ukip whose leadership had gone awol after the great opportunity offered by its third place at last june european ukip roger has said he is glad to see the back of mr he has remarkable ability to influence people after the election it became clear that he was more interested in the robert party than the uk independence party so it was nice knowing now he ukip officials also argue mr has not been straightforward in attacking the party he once wanted to\n",
      "Decoded sentence:  roger has said last saturday at least six nations because i am committed\n",
      "-\n",
      "Input sentence: donor attacks reported feud between tony blair and gordon brown has prompted a labour donor to say he will almost certainly refuse to give more bannatyne also attacked the government over iraq and its poor response to the asian tsunami his broadside came as secretary robin cook said he hoped mr brown would be premier at some mr bannatyne has previously given labour he made his fortune from care homes and health on tuesday said was he was reviewing his donations because of cabinet disunity and international his spokesman said it was highly unlikely he would give labour more although he would remain a supporter and not fund the peston new book has prompted more speculation about the rift with its claims that the prime minister broke a promise made in to stand mr bannatyne disunity in the cabinet has a corrosive effect on the gordon brown is a great chancellor who has delivered a stable but business wants that to continue and not be blown off course by petty squabbles based on personal the whose latest venture is a casino in also voiced concern about the ongoing violence in he branded the uk government response to the tsunami as piecemeal and the people there need practical help not just pledges of he the us has forces helping on the ground we can do british navy ships have helped the relief effort and the prime minister has said the government could ultimately give hundreds of millions of pounds in mr bannatyne is due to host a new television programme and is also appearing on business programme dragon but his spokesman insisted his attack on labour was not a publicity a separate robin cook gave his support to mr brown prime ministerial ambitions but told a lunch for political journalists winning the election had to be labour but he insisted the recent squabbles between mr blair and mr brown were not perceived as a problem by the adding there was no impression of governmental mr cook argued that more prominence was given to these matters because there was not an alternative source of opposition to the he warned the abstentions party was the real challenge to labour and they would not be motivated by mr blair promise to produce an unremittingly new labour election his comments come after dave the leader of britain biggest union told the daily record newspaper he wants a date to be set for mr blair to be replaced as labour\n",
      "Decoded sentence:  political editor has been named his <OOV> he said he was delighted with\n",
      "-\n",
      "Input sentence: research fears over kelly have expressed concerns that new education secretary ruth kelly religious views could hamper vital scientific who is is reported to be and has opposed embryo medical research council professor nancy rothwell said ms kelly views mattered as she was responsible for training future the department for education and skills would not comment on the spokeswoman it is not news that ms kelly is a catholic but we are not going into any details on but she added that claims ms kelly was in charge of a university research budget were not it was down to the higher education funding council and the research councils to decide on research british law is open to the cloning of human embryos to create stem master cells that can develop into all the body tissue this cloning activity is not permitted for reproductive purposes only for research into new disease it is controversial because it involves the destruction of who is also of research at manchester told the times higher education supplement it would worry her a great deal if ministers were she the views of ministers in the dfes do matter as they are responsible for training the next generation of you cannot have a higher education policy that is at odds with the government science head of developmental genetics at the national institute of medical professor robin said he had witnessed the confused situation in the us where many religious groups opposed the he if someone as senior as ruth kelly is not going to favour stem cell research we will end up with a similarly schizophrenic system in this it is very but fertility expert lord winston said he thought it was rather good ministers held ethical concerns have also been raised by organisations that ms kelly views might affect sex education policy in planning association chief anne said teaching pupils about contraception and abortion were young people must be informed about all the issues within sexual which include contraception and i think it is very important that the government maintains its commitment to the teenage pregnancy individual schools devise their own sex education policies based on a framework provided by the ms kelly has not set out her detailed views on either issue but has said she intends to put parents first in education this would include the quality of classroom discipline and academic standards in she\n",
      "Decoded sentence:  will play the <OOV> in the uk tour was one of the best actress award for\n",
      "-\n",
      "Input sentence: chancellor rallies labour brown has issued a rallying telling supporters the stakes are too high to stay at home or protest vote in the forthcoming general chancellor said the vote expected to fall on may will give a clear and fundamental choice between labour investment and conservative speaking at labour spring conference in mr brown claimed the nhs was not safe in conservative he said tory plans to cut tax would cut deep into public a packed audience at gateshead sage the chancellor said the cuts proposed by shadow chancellor oliver letwin were the equivalent of sacking every gp and nurse in the he told laying into the conservative record in government he i give you this promise with britain will never return to the mistakes of erm and interest in lost one million in negative equity and three million never again tory boom and will be the central dividing line at the between a conservative party taking britain back and planning deep cuts of in our and a labour government taking britain which on a platform of stability will reform and renew our schools and public services i am proud to spend by turning to the the chancellor pledged to continue economic stability and growth in a third term in said after seven years labour had transformed from a party not trusted with the economy to the only party trusted with the it was now a party not just of but of employers and he in the speech which prompted a standing ovation from an audience clearly warm to mr brown he also promised to end teenage unemployment within the next five he also highlighted plans for debt relief for the world poorest a national minimum wage for and and the creation of a network of children centres and flexibility in maternity the prime minister is to take part later on saturday in an interactive question and answer fielding queries sent in by text message and telephone as part of labour attempt to engage the public in their\n",
      "Decoded sentence:  the fourth chancellor said to the public but i think the studios over the\n",
      "-\n",
      "Input sentence: fox attacks blair tory blair lied when he took the uk to war so has no qualms about lying in the election say the liam fox was speaking after mr blair told labour members the tories offered a hard right dr fox told bbc if you are willing to lie about the reasons for going to i guess you are going to lie about anything at he would not discuss reports the party repaid to lord ashcroft after he predicted an election prime minister ratcheted up labour campaigning at the weekend with a helicopter tour of the country and his speech at the party spring he insisted he did not know the poll but it is widely expected to be what was seen as a highly personal speech in gateshead on mr blair i have the same passion and hunger as when i first walked through the door of downing he described his relationship with the public as starting then struggling to live up to the and reaching the point of raised voices and throwing he warned his supporters against it a fight for the future of our it is a fight that for britain and the people of britain we have to blair said that whether the public chose michael howard or mr it would result in a tory government not a labour government and a country that goes back and does not move dr fox accused mr blair and other cabinet ministers of telling lies about their policies and then attacking the what we learned at the weekend is what labour tactics are going to be and it is going to be fear and he told bbc the tory attacked labour six new pledges as vacuous and said mr blair was very worried voters would take revenge for his failure to dr fox refused to discuss weekend newspaper reports that the party had repaid to former tory treasurer lord ashcroft after he said the party could not win the we repay loans when they are due but do not comment to individual financial he insisting he enjoyed a warm and constructive relationship to lord lib dem leader charles kennedy is expected to attack mr blair words as he begins a nationwide tour on mr kennedy is accelerating lib dem election preparations this week as he visits dorset and he this is in the northern the contest is between labour and the liberal in southern and rural seats especially in the south west the principal contenders are the liberal democrats and the who are out of the running in scotland and the lib dems accuse mr blair of making a speech to labour delegates which will not help him regain public\n",
      "Decoded sentence:  says a of the award which represents university the bbc news and <OOV>\n",
      "-\n",
      "Input sentence: tories unveil quango blitz to abolish quangos have been unveiled by the conservatives as part of their effort to show how government red tape can be government units would also be scrapped under proposals which the tories say would save more than among the targets are strategic health authorities and the new fair access regulator for tory frontbencher john redwood said britain needed a slimmer government and lower taxes to be plans would abolish regional assemblies and other regional such as boards tackling industrial development and their powers would be returned to elected local councils or national the tories say the strategic health authorities are not needed as it is better that local rather than run hospitals and the mr redwood mr blair has forgotten the interests of and has broken the pledges he far from improving public spending money on quangos has led only to more more regulation and higher his party michael argued a change in direction was needed to get a grip on labour are creating two the britain of the forgotten majority and bureaucratic he in the real people are working harder just to stand they have seen their pensions knocked for being squeezed by extra the forgotten majority are paying the price of bureaucratic government has announced plans to cut civil servants as part of its efficiency but chief secretary to the treasury paul boateng attacked the tory the conservatives are committed to cutting labour public spending plans by a massive he cuts on this scale cannot be found from cutting but would require massive cuts to public services such as hospitals and the the liberal democrats have said they would cut the number of whitehall departments to make sure money reaches frontline\n",
      "Decoded sentence:  chief party also <OOV> his <OOV> was one of the <OOV> <OOV> in the us department\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    input_seq = training_x[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', dataset['Articles'][seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-28T10:56:15.603486Z",
     "iopub.status.busy": "2022-08-28T10:56:15.603091Z",
     "iopub.status.idle": "2022-08-28T10:56:15.642035Z",
     "shell.execute_reply": "2022-08-28T10:56:15.641140Z",
     "shell.execute_reply.started": "2022-08-28T10:56:15.603455Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model.save('encoder.h5')\n",
    "decoder_model.save('decoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
